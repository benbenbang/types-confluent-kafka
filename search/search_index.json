{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Types Confluent Kafka Documentation","text":"<p>Welcome to the <code>types-confluent-kafka</code> documentation! This package provides comprehensive type hints for the <code>confluent-kafka</code> Python package, enabling better development experience with type checking and auto-completion support.</p>"},{"location":"#what-is-types-confluent-kafka","title":"What is types-confluent-kafka?","text":"<p><code>types-confluent-kafka</code> is a package that provides type hints for the <code>confluent-kafka</code> Python package. It's designed to enhance your development experience by enabling type checking with tools like <code>mypy</code> and providing auto-completion support in your LSP (language server).</p> <p>Important Note</p> <p>Confluent does not officially endorse this package.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Complete Type Coverage: Comprehensive type hints for all confluent-kafka APIs</li> <li>IDE Support: Enhanced auto-completion and IntelliSense in your favorite IDE</li> <li>Type Safety: Catch type-related errors at development time with mypy and other type checkers</li> <li>LSP Integration: Works seamlessly with Language Server Protocol implementations</li> <li>Regular Updates: Frequent releases to stay up-to-date with confluent-kafka changes</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get started with <code>types-confluent-kafka</code> in just a few steps:</p> <ol> <li> <p>Install the package:    <pre><code>pip install --no-cache-dir types-confluent-kafka\n</code></pre></p> </li> <li> <p>Import and use with type hints:    <pre><code>from confluent_kafka import Producer, Consumer\n\n# Your IDE and type checker now understand the types!\nproducer: Producer = Producer({'bootstrap.servers': 'localhost:9092'})\nconsumer: Consumer = Consumer({'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'})\n</code></pre></p> </li> <li> <p>Enable type checking:    <pre><code>mypy your_kafka_application.py\n</code></pre></p> </li> </ol>"},{"location":"#compatibility","title":"Compatibility","text":"<p>For detailed compatibility information with different versions of confluent-kafka, please check the compatibility table.</p>"},{"location":"#versioning","title":"Versioning","text":"<p><code>types-confluent-kafka</code> follows Semantic Versioning 2.0.0:</p> <ul> <li>MAJOR version for incompatible API changes</li> <li>MINOR version for adding new features in a backward-compatible manner</li> <li>PATCH version for backward-compatible bug fixes</li> </ul> <p>We release frequently to provide you with the latest features and improvements. All commits follow conventional commits standard.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started - Detailed installation and setup guide</li> <li>Working with Typings - Learn how to effectively use the type hints</li> <li>Contributions &amp; Conventions - Contribute to the project</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache 2.0 License. You are free to use, modify, and distribute the code according to the terms of this license.</p>"},{"location":"contributions_and_conventions/","title":"Contributions &amp; Conventions","text":"<p>We welcome contributions to <code>types-confluent-kafka</code>! This guide outlines how to contribute effectively and the conventions we follow.</p>"},{"location":"contributions_and_conventions/#getting-started","title":"Getting Started","text":""},{"location":"contributions_and_conventions/#prerequisites","title":"Prerequisites","text":"<p>Before contributing, make sure you have:</p> <ul> <li>Python 3.9 or higher</li> <li>Git for version control</li> <li>A GitHub account</li> </ul>"},{"location":"contributions_and_conventions/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Fork and clone the repository:    <pre><code>git clone https://github.com/YOUR_USERNAME/types-confluent-kafka.git\ncd types-confluent-kafka\n</code></pre></p> </li> <li> <p>Set up the development environment:    <pre><code># Using uv (recommended)\nuv sync --group dev --group lint --group docs\n\n# Or using pip (install dev dependencies manually)\npip install --no-cache-dir -e .\npip install --no-cache-dir confluent-kafka[avro] mypy pyright pre-commit ruff mkdocs mkdocs-material\n</code></pre></p> </li> <li> <p>Install pre-commit hooks:    <pre><code>pre-commit install &amp;&amp; pre-commit install --hook-type commit-msg\n</code></pre></p> </li> <li> <p>Verify the setup:    <pre><code># Run type checking\nmypy confluent_kafka-stubs\npyright confluent_kafka-stubs\n\n# Run linting\nruff check .\nruff format --check .\n</code></pre></p> </li> </ol>"},{"location":"contributions_and_conventions/#contribution-workflow","title":"Contribution Workflow","text":""},{"location":"contributions_and_conventions/#1-create-an-issue-first","title":"1. Create an Issue First","text":"<p>Before making any changes:</p> <ol> <li>Check if an issue already exists for your proposed change</li> <li>If not, create a new issue describing:</li> <li>The problem you're solving</li> <li>Your proposed solution</li> <li>Any breaking changes</li> </ol>"},{"location":"contributions_and_conventions/#2-create-a-feature-branch","title":"2. Create a Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/your-bug-fix\n</code></pre>"},{"location":"contributions_and_conventions/#3-make-your-changes","title":"3. Make Your Changes","text":"<p>Follow the coding conventions outlined below and ensure your changes:</p> <ul> <li>Are focused and atomic</li> <li>Include appropriate type annotations</li> <li>Follow existing patterns in the codebase</li> <li>Don't break existing functionality</li> </ul>"},{"location":"contributions_and_conventions/#4-test-your-changes","title":"4. Test Your Changes","text":"<pre><code># Run type checking\nmypy confluent_kafka-stubs\npyright confluent_kafka-stubs\n\n# Run formatting and linting\nruff format .\nruff check .\n\n# Test with actual confluent-kafka if needed\npython -c \"import confluent_kafka; print('Import successful')\"\n</code></pre>"},{"location":"contributions_and_conventions/#5-commit-your-changes","title":"5. Commit Your Changes","text":"<p>We follow Conventional Commits:</p> <pre><code>git commit -m \"feat: add type hints for AdminClient.describe_configs\"\ngit commit -m \"fix: correct return type for Consumer.poll\"\ngit commit -m \"docs: update installation instructions\"\n</code></pre> <p>Commit Types: - <code>feat</code>: A new feature - <code>fix</code>: A bug fix - <code>docs</code>: Documentation only changes - <code>style</code>: Changes that do not affect the meaning of the code - <code>refactor</code>: A code change that neither fixes a bug nor adds a feature - <code>test</code>: Adding missing tests - <code>chore</code>: Changes to the build process or auxiliary tools</p>"},{"location":"contributions_and_conventions/#6-open-a-pull-request","title":"6. Open a Pull Request","text":"<ol> <li>Push your branch: <code>git push origin feature/your-feature-name</code></li> <li>Open a PR on GitHub</li> <li>Fill out the PR template completely</li> <li>Link to the related issue</li> <li>Request review from maintainers</li> </ol>"},{"location":"contributions_and_conventions/#coding-conventions","title":"Coding Conventions","text":""},{"location":"contributions_and_conventions/#type-stub-conventions","title":"Type Stub Conventions","text":""},{"location":"contributions_and_conventions/#file-organization","title":"File Organization","text":"<pre><code>confluent_kafka-stubs/\n\u001c\u0000\u0000 __init__.pyi              # Main module exports\n\u001c\u0000\u0000 _version.pyi              # Version information\n\u001c\u0000\u0000 admin/\n   \u001c\u0000\u0000 __init__.pyi         # Admin client types\n   \u0014\u0000\u0000 _admin.pyi           # Internal admin types\n\u001c\u0000\u0000 avro/\n   \u001c\u0000\u0000 __init__.pyi         # Avro serializer types\n   \u0014\u0000\u0000 serializer.pyi       # Avro serializer implementation\n\u001c\u0000\u0000 error.pyi                # Error classes\n\u001c\u0000\u0000 producer.pyi             # Producer types\n\u001c\u0000\u0000 consumer.pyi             # Consumer types\n\u0014\u0000\u0000 message.pyi              # Message types\n</code></pre>"},{"location":"contributions_and_conventions/#type-annotation-style","title":"Type Annotation Style","text":"<p>Class Definitions: <pre><code>class Producer:\n    def __init__(self, config: Dict[str, Any]) -&gt; None: ...\n\n    def produce(\n        self,\n        topic: str,\n        value: Optional[Union[str, bytes]] = ...,\n        key: Optional[Union[str, bytes]] = ...,\n        partition: int = ...,\n        on_delivery: Optional[Callable[[Optional[KafkaError], Message], None]] = ...,\n        timestamp: int = ...,\n        headers: Optional[Dict[str, Union[str, bytes]]] = ...,\n    ) -&gt; None: ...\n\n    def poll(self, timeout: float = ...) -&gt; int: ...\n    def flush(self, timeout: float = ...) -&gt; int: ...\n</code></pre></p> <p>Function Signatures: <pre><code># Use specific types over Any when possible\ndef good_function(config: Dict[str, Union[str, int, bool]]) -&gt; Producer: ...\n\n# Avoid generic Any unless necessary\ndef avoid_this(config: Any) -&gt; Any: ...\n\n# Use Union for multiple acceptable types\ndef flexible_input(\n    value: Union[str, bytes, int, None]\n) -&gt; Optional[str]: ...\n</code></pre></p> <p>Optional vs Required Parameters: <pre><code># Use ellipsis (...) for parameters with default values\ndef method(\n    required_param: str,\n    optional_param: Optional[str] = ...,\n    default_value_param: int = ...,\n) -&gt; None: ...\n</code></pre></p>"},{"location":"contributions_and_conventions/#documentation-in-stubs","title":"Documentation in Stubs","text":"<p>While stub files don't include implementation, you can add docstrings for complex types:</p> <pre><code>class ComplexConfig(TypedDict, total=False):\n    \"\"\"Configuration dictionary for Kafka clients.\n\n    This TypedDict defines the structure for configuration\n    options passed to Kafka clients.\n    \"\"\"\n    bootstrap_servers: str\n    client_id: str\n    # ... other fields\n</code></pre>"},{"location":"contributions_and_conventions/#code-quality-standards","title":"Code Quality Standards","text":""},{"location":"contributions_and_conventions/#type-checking","title":"Type Checking","text":"<p>All stub files must pass both mypy and pyright:</p> <pre><code># Must pass without errors\nmypy confluent_kafka-stubs\npyright confluent_kafka-stubs\n</code></pre>"},{"location":"contributions_and_conventions/#formatting","title":"Formatting","text":"<p>We use Ruff for formatting and linting:</p> <pre><code># Format code\nruff format .\n\n# Check for issues\nruff check .\n</code></pre>"},{"location":"contributions_and_conventions/#import-organization","title":"Import Organization","text":"<p>Organize imports in this order:</p> <pre><code># 1. Standard library imports\nfrom typing import Any, Dict, List, Optional, Union, Callable\nfrom typing_extensions import TypedDict  # For Python &lt; 3.8 compat\n\n# 2. Related third party imports (if any)\n\n# 3. Local application/library specific imports\nfrom . import _error\nfrom ._version import __version__\n</code></pre>"},{"location":"contributions_and_conventions/#testing-conventions","title":"Testing Conventions","text":""},{"location":"contributions_and_conventions/#type-checking-tests","title":"Type Checking Tests","text":"<p>Create test files to verify type annotations work correctly:</p> <pre><code># tests/test_producer_types.py\nfrom confluent_kafka import Producer\nfrom typing import Dict, Any\n\ndef test_producer_creation() -&gt; None:\n    config: Dict[str, Any] = {'bootstrap.servers': 'localhost:9092'}\n    producer: Producer = Producer(config)\n\n    # Test method calls with proper types\n    producer.produce('topic', value='message')\n    producer.produce('topic', value=b'binary_message', key='key')\n</code></pre>"},{"location":"contributions_and_conventions/#real-world-usage-tests","title":"Real-world Usage Tests","text":"<p>Test with actual confluent-kafka library when possible:</p> <pre><code># tests/test_integration.py\ndef test_types_with_real_library() -&gt; None:\n    \"\"\"Test that our types work with the actual library.\"\"\"\n    try:\n        import confluent_kafka\n\n        # Verify our types match reality\n        producer = confluent_kafka.Producer({'bootstrap.servers': 'test'})\n\n        # This should not raise type errors\n        producer.produce('topic', 'value')\n\n    except ImportError:\n        pytest.skip(\"confluent-kafka not available\")\n</code></pre>"},{"location":"contributions_and_conventions/#documentation-conventions","title":"Documentation Conventions","text":""},{"location":"contributions_and_conventions/#docstring-style","title":"Docstring Style","text":"<p>For complex types or configurations, provide clear documentation:</p> <pre><code>class ConsumerConfig(TypedDict, total=False):\n    \"\"\"Configuration options for Kafka Consumer.\n\n    Args:\n        bootstrap_servers: Comma-separated list of broker addresses\n        group_id: Consumer group identifier\n        auto_offset_reset: What to do when there is no initial offset\n        enable_auto_commit: Whether to automatically commit offsets\n    \"\"\"\n    bootstrap_servers: str\n    group_id: str\n    auto_offset_reset: Literal['earliest', 'latest', 'none']\n    enable_auto_commit: bool\n</code></pre>"},{"location":"contributions_and_conventions/#readme-updates","title":"README Updates","text":"<p>When adding significant features, update relevant documentation:</p> <ul> <li>Update the main README.md if needed</li> <li>Add examples to demonstrate new functionality</li> <li>Update compatibility information</li> </ul>"},{"location":"contributions_and_conventions/#version-compatibility","title":"Version Compatibility","text":""},{"location":"contributions_and_conventions/#maintaining-compatibility","title":"Maintaining Compatibility","text":"<ul> <li>Backward Compatibility: New type hints should not break existing code</li> <li>Forward Compatibility: Consider future confluent-kafka versions</li> <li>Python Compatibility: Support Python 3.9+ as defined in pyproject.toml</li> </ul>"},{"location":"contributions_and_conventions/#version-testing","title":"Version Testing","text":"<p>Test against multiple confluent-kafka versions when possible:</p> <pre><code># Test with minimum supported version\npip install confluent-kafka==2.5.0\npython -c \"import confluent_kafka\"\n\n# Test with latest version\npip install --upgrade confluent-kafka\npython -c \"import confluent_kafka\"\n</code></pre>"},{"location":"contributions_and_conventions/#release-process","title":"Release Process","text":""},{"location":"contributions_and_conventions/#semantic-versioning","title":"Semantic Versioning","text":"<p>We follow SemVer:</p> <ul> <li>MAJOR: Incompatible API changes</li> <li>MINOR: New functionality in a backward-compatible manner</li> <li>PATCH: Backward-compatible bug fixes</li> </ul>"},{"location":"contributions_and_conventions/#automated-releases","title":"Automated Releases","text":"<p>Releases are automated through semantic-release based on conventional commits:</p> <ol> <li>Merge PR to main branch</li> <li>CI validates all checks pass</li> <li>semantic-release analyzes commits and creates release</li> <li>Package is automatically published to PyPI</li> </ol>"},{"location":"contributions_and_conventions/#community-guidelines","title":"Community Guidelines","text":""},{"location":"contributions_and_conventions/#code-of-conduct","title":"Code of Conduct","text":"<p>This project follows the Contributor Covenant Code of Conduct. Be respectful and inclusive in all interactions.</p>"},{"location":"contributions_and_conventions/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: Use GitHub issues for bug reports and feature requests</li> <li>Discussions: Use GitHub Discussions for questions and general discussion</li> <li>Pull Requests: Use PR reviews for code-specific discussions</li> </ul>"},{"location":"contributions_and_conventions/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - Release notes for their contributions - GitHub contributor graph - Acknowledgments in major releases</p> <p>Thank you for contributing to <code>types-confluent-kafka</code>! Your efforts help make Kafka development in Python more reliable and enjoyable for everyone.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This guide will help you get up and running with <code>types-confluent-kafka</code> in your Python projects.</p>"},{"location":"getting_started/#installation","title":"Installation","text":""},{"location":"getting_started/#using-pip","title":"Using pip","text":"<p>The recommended way to install <code>types-confluent-kafka</code> is from PyPI:</p> <pre><code>pip install --no-cache-dir types-confluent-kafka\n</code></pre>"},{"location":"getting_started/#using-uv","title":"Using uv","text":"<p>If you're using uv as your package manager:</p> <pre><code># Add to development dependencies\nuv add --group dev types-confluent-kafka\n\n# Or add to regular dependencies\nuv add types-confluent-kafka\n</code></pre>"},{"location":"getting_started/#using-poetry","title":"Using Poetry","text":"<p>For Poetry users:</p> <pre><code># Add to development dependencies\npoetry add --group dev types-confluent-kafka\n\n# Or add to regular dependencies\npoetry add types-confluent-kafka\n</code></pre>"},{"location":"getting_started/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher</li> <li><code>confluent-kafka</code> package (the actual runtime library)</li> </ul>"},{"location":"getting_started/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify everything is working correctly:</p> <pre><code># Check that packages are installed\npip list | grep confluent\n\n# Verify no dependency conflicts\npip check\n</code></pre> <p>You can also verify that the types are working by creating a simple test file:</p> <pre><code># test_types.py\nfrom confluent_kafka import Producer, Consumer, KafkaError\n\n# These should now have proper type hints in your IDE\nproducer: Producer = Producer({'bootstrap.servers': 'localhost:9092'})\nconsumer: Consumer = Consumer({\n    'bootstrap.servers': 'localhost:9092',\n    'group.id': 'test-group',\n    'auto.offset.reset': 'earliest'\n})\n\n# Type checker should understand the return types\ndef handle_kafka_error(error: KafkaError) -&gt; None:\n    print(f\"Kafka error: {error}\")\n</code></pre>"},{"location":"getting_started/#ide-configuration","title":"IDE Configuration","text":""},{"location":"getting_started/#vs-code","title":"VS Code","text":"<p>If you're using VS Code with the Python extension, type hints should work automatically. Make sure you have:</p> <ol> <li>Python extension installed</li> <li>Pylance language server enabled (default)</li> <li>Type checking mode set to \"basic\" or \"strict\" in settings</li> </ol>"},{"location":"getting_started/#pycharm","title":"PyCharm","text":"<p>PyCharm should automatically recognize the type hints. You can enable stricter type checking by:</p> <ol> <li>Going to Settings \u2192 Editor \u2192 Inspections</li> <li>Enabling \"Python \u2192 Type checker\" inspections</li> </ol>"},{"location":"getting_started/#zed","title":"Zed","text":"<p>Zed has built-in Python LSP support that automatically recognizes type hints:</p> <ol> <li>Install Zed from zed.dev</li> <li>Open your Python project</li> <li>The Python LSP is included with Zed - no additional setup needed</li> <li>Type hints should work automatically</li> </ol>"},{"location":"getting_started/#other-ides","title":"Other IDEs","text":"<p>Most modern Python IDEs with LSP support will automatically pick up the type hints.</p>"},{"location":"getting_started/#type-checker-integration","title":"Type Checker Integration","text":""},{"location":"getting_started/#mypy","title":"mypy","text":"<p>Add mypy to your development dependencies and run it on your code:</p> <pre><code>pip install --no-cache-dir mypy\nmypy your_kafka_code.py\n</code></pre>"},{"location":"getting_started/#pyright","title":"pyright","text":"<p>If you prefer pyright:</p> <pre><code>pip install --no-cache-dir pyright\npyright your_kafka_code.py\n</code></pre>"},{"location":"getting_started/#basic-usage-example","title":"Basic Usage Example","text":"<p>Here's a simple producer/consumer example with full type safety:</p> <pre><code>from confluent_kafka import Producer, Consumer, KafkaError, KafkaException\nfrom typing import Optional\n\ndef create_producer() -&gt; Producer:\n    \"\"\"Create a Kafka producer with type safety.\"\"\"\n    config = {\n        'bootstrap.servers': 'localhost:9092',\n        'client.id': 'my-producer'\n    }\n    return Producer(config)\n\ndef create_consumer() -&gt; Consumer:\n    \"\"\"Create a Kafka consumer with type safety.\"\"\"\n    config = {\n        'bootstrap.servers': 'localhost:9092',\n        'group.id': 'my-consumer-group',\n        'auto.offset.reset': 'earliest'\n    }\n    return Consumer(config)\n\ndef produce_message(producer: Producer, topic: str, message: str) -&gt; None:\n    \"\"\"Produce a message with proper error handling.\"\"\"\n    try:\n        producer.produce(topic, value=message.encode('utf-8'))\n        producer.flush()\n    except KafkaException as e:\n        print(f\"Failed to produce message: {e}\")\n\ndef consume_messages(consumer: Consumer, topics: list[str]) -&gt; None:\n    \"\"\"Consume messages with proper type hints.\"\"\"\n    consumer.subscribe(topics)\n\n    try:\n        while True:\n            msg = consumer.poll(timeout=1.0)\n\n            if msg is None:\n                continue\n\n            if msg.error():\n                error: KafkaError = msg.error()\n                if error.code() == KafkaError._PARTITION_EOF:\n                    print(\"End of partition reached\")\n                else:\n                    print(f\"Consumer error: {error}\")\n            else:\n                value: Optional[bytes] = msg.value()\n                if value:\n                    print(f\"Received: {value.decode('utf-8')}\")\n\n    except KeyboardInterrupt:\n        pass\n    finally:\n        consumer.close()\n\n# Usage\nif __name__ == \"__main__\":\n    producer = create_producer()\n    consumer = create_consumer()\n\n    # Your IDE will provide auto-completion and type checking\n    produce_message(producer, \"test-topic\", \"Hello, Kafka!\")\n    consume_messages(consumer, [\"test-topic\"])\n</code></pre>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<p>Now that you have <code>types-confluent-kafka</code> installed and working:</p> <ul> <li>Learn more about Working with Typings</li> <li>Check out Troubleshooting if you encounter any issues</li> <li>Consider contributing to the project</li> </ul>"},{"location":"getting_started/#common-issues","title":"Common Issues","text":""},{"location":"getting_started/#type-hints-not-showing-up","title":"Type hints not showing up","text":"<ol> <li>Make sure you've installed <code>types-confluent-kafka</code> in the same environment as your project</li> <li>Restart your IDE/language server</li> <li>Check that your Python interpreter is correctly configured</li> </ol>"},{"location":"getting_started/#import-errors","title":"Import errors","text":"<p>If you get import errors, ensure that both <code>confluent-kafka</code> and <code>types-confluent-kafka</code> are installed:</p> <pre><code>pip list | grep confluent\npip check\n</code></pre> <p>You should see both packages listed and <code>pip check</code> should report no dependency conflicts.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>This guide helps you resolve common issues when using <code>types-confluent-kafka</code>.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#package-not-found","title":"Package Not Found","text":"<p>Problem: <code>pip install --no-cache-dir types-confluent-kafka</code> fails with \"package not found\"</p> <p>Solutions: <pre><code># Ensure you're using the correct package name\npip install --no-cache-dir types-confluent-kafka\n\n# Try with explicit index\npip install --index-url https://pypi.org/simple/ types-confluent-kafka\n\n# Clear pip cache if needed\npip cache purge\npip install --no-cache-dir types-confluent-kafka\n</code></pre></p>"},{"location":"troubleshooting/#version-conflicts","title":"Version Conflicts","text":"<p>Problem: Dependency conflicts during installation</p> <p>Solutions: <pre><code># Check current confluent-kafka version\npip show confluent-kafka\n\n# Install compatible version\npip install --no-cache-dir \"confluent-kafka&gt;=2.5.0\"\n\n# Use virtual environment to avoid conflicts\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install --no-cache-dir types-confluent-kafka\n</code></pre></p>"},{"location":"troubleshooting/#type-checking-issues","title":"Type Checking Issues","text":""},{"location":"troubleshooting/#types-not-recognized","title":"Types Not Recognized","text":"<p>Problem: IDE or type checker doesn't recognize the type hints</p> <p>Diagnosis: <pre><code># Check if package is installed\npip show types-confluent-kafka\n\n# Verify Python path\npython -c \"import sys; print(sys.path)\"\n\n# Check if stubs are found\npython -c \"import confluent_kafka; print(confluent_kafka.__file__)\"\n</code></pre></p> <p>Solutions:</p> <ol> <li>Restart your IDE/Language Server:</li> <li>VS Code: Reload window (Ctrl/Cmd + Shift + P \u2192 \"Developer: Reload Window\")</li> <li> <p>PyCharm: File \u2192 Invalidate Caches and Restart</p> </li> <li> <p>Check Python interpreter:    <pre><code># Ensure same environment\nwhich python\nwhich pip\n\n# In VS Code, check Python interpreter path\n# Ctrl/Cmd + Shift + P \u2192 \"Python: Select Interpreter\"\n</code></pre></p> </li> <li> <p>Force reinstall:    <pre><code>pip uninstall types-confluent-kafka\npip install --no-cache-dir types-confluent-kafka\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#mypy-errors","title":"Mypy Errors","text":"<p>Problem: <code>mypy</code> reports errors with confluent-kafka types</p> <p>Common Error: <code>Module has no attribute</code> <pre><code># Error example\nerror: Module \"confluent_kafka\" has no attribute \"Producer\"\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check mypy configuration:    <pre><code># pyproject.toml\n[tool.mypy]\npython_version = \"3.9\"\nignore_missing_imports = false\n</code></pre></p> </li> <li> <p>Explicit type checking:    <pre><code># Add at top of file\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from confluent_kafka import Producer, Consumer\nelse:\n    import confluent_kafka\n    Producer = confluent_kafka.Producer\n    Consumer = confluent_kafka.Consumer\n</code></pre></p> </li> <li> <p>Update mypy:    <pre><code>pip install --no-cache-dir --upgrade mypy\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#pyrightpylance-issues","title":"Pyright/Pylance Issues","text":"<p>Problem: Pyright/Pylance doesn't find types</p> <p>Solutions:</p> <ol> <li> <p>Check pyrightconfig.json:    <pre><code>{\n  \"include\": [\"src\"],\n  \"stubPath\": \"./typings\",\n  \"typeCheckingMode\": \"basic\"\n}\n</code></pre></p> </li> <li> <p>Update Pylance:</p> </li> <li> <p>VS Code \u2192 Extensions \u2192 Python \u2192 Update</p> </li> <li> <p>Clear cache:    <pre><code># Clear Pylance cache\n# VS Code: Ctrl/Cmd + Shift + P \u2192 \"Python: Clear Cache\"\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#runtime-issues","title":"Runtime Issues","text":""},{"location":"troubleshooting/#import-errors","title":"Import Errors","text":"<p>Problem: <code>ImportError</code> when importing confluent_kafka</p> <p>Error: <pre><code>ImportError: No module named 'confluent_kafka'\n</code></pre></p> <p>Solution: <pre><code># Install the runtime library\npip install --no-cache-dir confluent-kafka\n\n# Or with Avro support\npip install --no-cache-dir \"confluent-kafka[avro]\"\n\n# Verify installation\npython -c \"import confluent_kafka; print('Success')\"\n</code></pre></p>"},{"location":"troubleshooting/#version-mismatch","title":"Version Mismatch","text":"<p>Problem: Type hints don't match your confluent-kafka version</p> <p>Diagnosis: <pre><code>import confluent_kafka\nprint(f\"confluent-kafka version: {confluent_kafka.__version__}\")\n\nimport types_confluent_kafka  # This will fail, it's just type stubs\n# Check pip show instead\n</code></pre></p> <pre><code>pip show confluent-kafka types-confluent-kafka\n</code></pre> <p>Solutions:</p> <ol> <li> <p>Check compatibility table: See our compatibility documentation</p> </li> <li> <p>Update to compatible versions:    <pre><code># Update confluent-kafka\npip install --no-cache-dir --upgrade confluent-kafka\n\n# Update types\npip install --no-cache-dir --upgrade types-confluent-kafka\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#ide-specific-issues","title":"IDE-Specific Issues","text":""},{"location":"troubleshooting/#vs-code","title":"VS Code","text":"<p>Problem: Auto-completion not working</p> <p>Solutions: 1. Check Python extension:    - Ensure Python extension is installed and enabled    - Update to latest version</p> <ol> <li>Verify interpreter:</li> <li>Ctrl/Cmd + Shift + P \u2192 \"Python: Select Interpreter\"</li> <li> <p>Choose the environment where types-confluent-kafka is installed</p> </li> <li> <p>Enable Pylance:    <pre><code>// settings.json\n{\n  \"python.languageServer\": \"Pylance\",\n  \"python.analysis.typeCheckingMode\": \"basic\"\n}\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#pycharm","title":"PyCharm","text":"<p>Problem: Type hints not showing</p> <p>Solutions: 1. Check interpreter: File \u2192 Settings \u2192 Project \u2192 Python Interpreter</p> <ol> <li> <p>Enable type checking: Settings \u2192 Editor \u2192 Inspections \u2192 Python \u2192 Type checker</p> </li> <li> <p>Rebuild index: File \u2192 Invalidate Caches and Restart</p> </li> </ol>"},{"location":"troubleshooting/#vimneovim","title":"Vim/Neovim","text":"<p>Problem: LSP not recognizing types</p> <p>Solutions: 1. Check LSP server: Ensure pyright or pylsp is installed    <pre><code># For pyright\nnpm install -g pyright\n\n# For pylsp\npip install --no-cache-dir python-lsp-server\n</code></pre></p> <ol> <li>LSP configuration: Verify your LSP is configured to use the correct Python environment</li> </ol>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#slow-type-checking","title":"Slow Type Checking","text":"<p>Problem: Type checking is very slow</p> <p>Solutions:</p> <ol> <li> <p>Exclude unnecessary files:    <pre><code># pyproject.toml\n[tool.mypy]\nexclude = [\n    \"build/\",\n    \"dist/\",\n    \".venv/\",\n    \"tests/\",\n]\n</code></pre></p> </li> <li> <p>Use incremental checking:    <pre><code>[tool.mypy]\nincremental = true\ncache_dir = \".mypy_cache\"\n</code></pre></p> </li> <li> <p>Limit scope:    <pre><code># Check specific files only\nmypy src/my_kafka_app.py\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#configuration-issues","title":"Configuration Issues","text":""},{"location":"troubleshooting/#type-checking-configuration","title":"Type Checking Configuration","text":"<p>Problem: Type checker reports false positives</p> <p>Example Configuration: <pre><code># pyproject.toml\n[tool.mypy]\npython_version = \"3.9\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\nignore_missing_imports = false\n\n# Specific to confluent-kafka\n[[tool.mypy.overrides]]\nmodule = \"confluent_kafka.*\"\nignore_missing_imports = false\n</code></pre></p>"},{"location":"troubleshooting/#pyright-configuration","title":"Pyright Configuration","text":"<pre><code>{\n  \"include\": [\"src\"],\n  \"exclude\": [\"**/node_modules\", \"**/__pycache__\", \"**/.*\"],\n  \"reportMissingImports\": \"warning\",\n  \"reportMissingTypeStubs\": \"information\",\n  \"pythonVersion\": \"3.9\",\n  \"typeCheckingMode\": \"basic\"\n}\n</code></pre>"},{"location":"troubleshooting/#common-patterns-and-workarounds","title":"Common Patterns and Workarounds","text":""},{"location":"troubleshooting/#working-with-optional-types","title":"Working with Optional Types","text":"<p>Problem: Dealing with Optional return types</p> <pre><code>from confluent_kafka import Consumer\nfrom typing import Optional\n\nconsumer = Consumer({'bootstrap.servers': 'localhost:9092'})\n\n# This might be None\nmsg: Optional[Message] = consumer.poll(1.0)\n\n# Type-safe handling\nif msg is not None:\n    # Now mypy knows msg is not None\n    if not msg.error():\n        value = msg.value()\n        if value is not None:\n            print(value.decode('utf-8'))\n</code></pre>"},{"location":"troubleshooting/#configuration-typing","title":"Configuration Typing","text":"<p>Problem: Configuration dictionaries are not type-safe</p> <p>Solution: Use TypedDict</p> <pre><code>from typing import TypedDict, Literal\n\nclass ProducerConfig(TypedDict, total=False):\n    bootstrap_servers: str\n    client_id: str\n    acks: Literal['0', '1', 'all']\n\nconfig: ProducerConfig = {\n    'bootstrap_servers': 'localhost:9092',\n    'acks': 'all'  # Type-safe: only accepts valid values\n}\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#before-asking-for-help","title":"Before Asking for Help","text":"<ol> <li>Check this troubleshooting guide</li> <li>Search existing GitHub issues: Issues page</li> <li>Verify versions: Ensure compatibility between confluent-kafka and types-confluent-kafka</li> <li>Minimal reproduction: Create a minimal example that demonstrates the issue</li> </ol>"},{"location":"troubleshooting/#reporting-issues","title":"Reporting Issues","text":"<p>When reporting issues, include:</p> <ol> <li> <p>Environment information:    <pre><code>python --version\npip show confluent-kafka types-confluent-kafka\n\n# For type checker issues\nmypy --version\n# or\npyright --version\n</code></pre></p> </li> <li> <p>Minimal code example:    <pre><code>from confluent_kafka import Producer\n\n# Code that demonstrates the issue\nproducer = Producer({'bootstrap.servers': 'localhost:9092'})\n</code></pre></p> </li> <li> <p>Error messages: Full error output and stack traces</p> </li> <li> <p>IDE/Editor: Name and version if relevant</p> </li> </ol>"},{"location":"troubleshooting/#community-support","title":"Community Support","text":"<ul> <li>GitHub Issues: For bugs and feature requests</li> <li>GitHub Discussions: For questions and general help</li> <li>Stack Overflow: Tag with <code>confluent-kafka</code> and <code>python-typing</code></li> </ul>"},{"location":"troubleshooting/#when-all-else-fails","title":"When All Else Fails","text":"<p>If you've checked everything in this guide and still have issues, there might be something missing or wrong in this typing package itself! In that case:</p> <ol> <li>Report an Issue: Create a GitHub issue with:</li> <li>Your environment details (Python version, package versions)</li> <li>Minimal code example that reproduces the problem</li> <li> <p>Expected vs actual behavior</p> </li> <li> <p>Contribute a Fix: Even better, if you know how to fix it:</p> </li> <li>Fork the repository</li> <li>Create a fix following our contribution guidelines</li> <li>Submit a pull request with the solution</li> </ol> <p>Remember: <code>types-confluent-kafka</code> only provides type hints. Runtime issues with Kafka operations should be directed to the confluent-kafka-python project.</p>"},{"location":"work_with_typings/","title":"Working with Typings","text":"<p>This guide covers how to effectively use the type hints provided by <code>types-confluent-kafka</code> to write better, more reliable Kafka applications.</p>"},{"location":"work_with_typings/#understanding-the-type-system","title":"Understanding the Type System","text":"<p><code>types-confluent-kafka</code> provides comprehensive type annotations for all major confluent-kafka classes and functions. This includes:</p> <ul> <li>Producer and Consumer classes with their configuration options</li> <li>Message objects with proper typing for keys, values, headers, and metadata</li> <li>Error classes with specific error code types</li> <li>Administrative client types for cluster management</li> <li>Serialization interfaces and implementations</li> </ul>"},{"location":"work_with_typings/#core-type-patterns","title":"Core Type Patterns","text":""},{"location":"work_with_typings/#producer-types","title":"Producer Types","text":"<pre><code>from confluent_kafka import Producer\nfrom typing import Dict, Any, Optional, Callable\n\n# Producer configuration is typed as a dictionary\nproducer_config: Dict[str, Any] = {\n    'bootstrap.servers': 'localhost:9092',\n    'client.id': 'my-producer',\n    'acks': 'all',\n    'retries': 3\n}\n\nproducer: Producer = Producer(producer_config)\n\n# Delivery callback with proper typing\ndef delivery_callback(err: Optional[Exception], msg: 'Message') -&gt; None:\n    if err:\n        print(f\"Delivery failed: {err}\")\n    else:\n        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n\n# Produce with type safety\nproducer.produce(\n    topic='my-topic',\n    key='user-123',\n    value=b'{\"event\": \"user_login\"}',\n    on_delivery=delivery_callback\n)\n</code></pre>"},{"location":"work_with_typings/#consumer-types","title":"Consumer Types","text":"<pre><code>from confluent_kafka import Consumer, Message, KafkaError\nfrom typing import List, Optional\n\nconsumer_config: Dict[str, Any] = {\n    'bootstrap.servers': 'localhost:9092',\n    'group.id': 'my-consumer-group',\n    'auto.offset.reset': 'earliest'\n}\n\nconsumer: Consumer = Consumer(consumer_config)\n\ndef process_message(msg: Message) -&gt; None:\n    \"\"\"Process a Kafka message with full type safety.\"\"\"\n\n    # All message methods are properly typed\n    topic: str = msg.topic()\n    partition: int = msg.partition()\n    offset: int = msg.offset()\n    timestamp: tuple[int, int] = msg.timestamp()\n\n    # Optional values are properly typed\n    key: Optional[bytes] = msg.key()\n    value: Optional[bytes] = msg.value()\n    headers: Optional[List[tuple[str, bytes]]] = msg.headers()\n\n    # Error handling with proper types\n    error: Optional[KafkaError] = msg.error()\n    if error:\n        print(f\"Message error: {error}\")\n        return\n\n    if value:\n        print(f\"Processing: {value.decode('utf-8')}\")\n\n# Subscribe with proper typing\ntopics: List[str] = ['topic1', 'topic2']\nconsumer.subscribe(topics)\n\n# Poll loop with type safety\nwhile True:\n    msg: Optional[Message] = consumer.poll(timeout=1.0)\n    if msg is not None:\n        process_message(msg)\n</code></pre>"},{"location":"work_with_typings/#error-handling-types","title":"Error Handling Types","text":"<pre><code>from confluent_kafka import KafkaError, KafkaException\nfrom confluent_kafka.error import ConsumeError, ProduceError\n\ndef handle_kafka_errors(error: KafkaError) -&gt; None:\n    \"\"\"Handle different types of Kafka errors.\"\"\"\n\n    # Error codes are properly typed\n    error_code: int = error.code()\n    error_name: str = error.name()\n    error_str: str = str(error)\n\n    # Check for specific error conditions\n    if error.code() == KafkaError._PARTITION_EOF:\n        print(\"Reached end of partition\")\n    elif error.code() == KafkaError._UNKNOWN_TOPIC_OR_PART:\n        print(\"Unknown topic or partition\")\n    else:\n        print(f\"Kafka error: {error_name} - {error_str}\")\n\n# Exception handling with proper types\ntry:\n    producer.produce('topic', 'message')\nexcept KafkaException as e:\n    kafka_error: KafkaError = e.args[0]\n    handle_kafka_errors(kafka_error)\n</code></pre>"},{"location":"work_with_typings/#advanced-typing-patterns","title":"Advanced Typing Patterns","text":""},{"location":"work_with_typings/#generic-message-processing","title":"Generic Message Processing","text":"<pre><code>from typing import TypeVar, Generic, Protocol, Callable\nfrom confluent_kafka import Message\n\n# Define a protocol for serializable data\nclass Serializable(Protocol):\n    def serialize(self) -&gt; bytes: ...\n    @classmethod\n    def deserialize(cls, data: bytes) -&gt; 'Serializable': ...\n\nT = TypeVar('T', bound=Serializable)\n\nclass TypedMessageProcessor(Generic[T]):\n    \"\"\"A generic message processor with type safety.\"\"\"\n\n    def __init__(self, data_class: type[T]):\n        self.data_class = data_class\n\n    def process_message(self, msg: Message) -&gt; Optional[T]:\n        \"\"\"Process a message and return typed data.\"\"\"\n        if msg.error():\n            return None\n\n        value = msg.value()\n        if not value:\n            return None\n\n        try:\n            return self.data_class.deserialize(value)\n        except Exception as e:\n            print(f\"Deserialization error: {e}\")\n            return None\n\n# Usage with proper typing\nfrom dataclasses import dataclass\nimport json\n\n@dataclass\nclass UserEvent:\n    user_id: str\n    event_type: str\n    timestamp: int\n\n    def serialize(self) -&gt; bytes:\n        return json.dumps(self.__dict__).encode()\n\n    @classmethod\n    def deserialize(cls, data: bytes) -&gt; 'UserEvent':\n        return cls(**json.loads(data.decode()))\n\nprocessor: TypedMessageProcessor[UserEvent] = TypedMessageProcessor(UserEvent)\n</code></pre>"},{"location":"work_with_typings/#configuration-type-safety","title":"Configuration Type Safety","text":"<pre><code>from typing import TypedDict, Literal\n\n# Define typed configuration dictionaries\nclass ProducerConfig(TypedDict, total=False):\n    bootstrap_servers: str\n    client_id: str\n    acks: Literal['0', '1', 'all']\n    retries: int\n    batch_size: int\n    linger_ms: int\n    compression_type: Literal['none', 'gzip', 'snappy', 'lz4', 'zstd']\n\nclass ConsumerConfig(TypedDict, total=False):\n    bootstrap_servers: str\n    group_id: str\n    client_id: str\n    auto_offset_reset: Literal['earliest', 'latest', 'none']\n    enable_auto_commit: bool\n    auto_commit_interval_ms: int\n    session_timeout_ms: int\n\n# Type-safe configuration\ndef create_typed_producer(config: ProducerConfig) -&gt; Producer:\n    \"\"\"Create a producer with type-safe configuration.\"\"\"\n    # Convert snake_case to kafka's dot notation\n    kafka_config = {\n        key.replace('_', '.'): value\n        for key, value in config.items()\n    }\n    return Producer(kafka_config)\n\n# Usage\nproducer_config: ProducerConfig = {\n    'bootstrap_servers': 'localhost:9092',\n    'client_id': 'my-producer',\n    'acks': 'all',  # Type checker ensures only valid values\n    'compression_type': 'gzip'\n}\n\nproducer = create_typed_producer(producer_config)\n</code></pre>"},{"location":"work_with_typings/#admin-client-types","title":"Admin Client Types","text":"<pre><code>from confluent_kafka.admin import AdminClient, NewTopic, ConfigResource\nfrom typing import Dict, List\n\n# Admin client with proper typing\nadmin_config: Dict[str, Any] = {\n    'bootstrap.servers': 'localhost:9092'\n}\n\nadmin: AdminClient = AdminClient(admin_config)\n\n# Topic creation with type safety\ndef create_topics(topic_names: List[str], num_partitions: int = 1) -&gt; None:\n    \"\"\"Create topics with proper type annotations.\"\"\"\n\n    new_topics: List[NewTopic] = [\n        NewTopic(\n            topic=name,\n            num_partitions=num_partitions,\n            replication_factor=1\n        )\n        for name in topic_names\n    ]\n\n    # Create topics operation returns a dict with futures\n    futures = admin.create_topics(new_topics)\n\n    # Wait for operations to complete\n    for topic_name, future in futures.items():\n        try:\n            future.result()  # The result() call blocks until completion\n            print(f\"Topic {topic_name} created successfully\")\n        except Exception as e:\n            print(f\"Failed to create topic {topic_name}: {e}\")\n</code></pre>"},{"location":"work_with_typings/#type-checking-best-practices","title":"Type Checking Best Practices","text":""},{"location":"work_with_typings/#use-type-checkers","title":"Use Type Checkers","text":"<p>Enable strict type checking in your development workflow:</p> <pre><code># mypy configuration in pyproject.toml\n[tool.mypy]\nstrict = true\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\n</code></pre>"},{"location":"work_with_typings/#type-annotations-in-function-signatures","title":"Type Annotations in Function Signatures","text":"<p>Always annotate function parameters and return types:</p> <pre><code>from confluent_kafka import Producer, Consumer, Message\nfrom typing import Optional, List, Dict, Any\n\ndef create_producer(\n    bootstrap_servers: str,\n    client_id: Optional[str] = None,\n    **kwargs: Any\n) -&gt; Producer:\n    \"\"\"Create a Kafka producer with type-safe parameters.\"\"\"\n    config: Dict[str, Any] = {\n        'bootstrap.servers': bootstrap_servers,\n        **kwargs\n    }\n    if client_id:\n        config['client.id'] = client_id\n\n    return Producer(config)\n\ndef batch_consume(\n    consumer: Consumer,\n    batch_size: int = 100,\n    timeout: float = 1.0\n) -&gt; List[Message]:\n    \"\"\"Consume messages in batches with proper typing.\"\"\"\n    messages: List[Message] = []\n\n    for _ in range(batch_size):\n        msg: Optional[Message] = consumer.poll(timeout)\n        if msg is None:\n            break\n        messages.append(msg)\n\n    return messages\n</code></pre>"},{"location":"work_with_typings/#handling-optional-values","title":"Handling Optional Values","text":"<p>Many Kafka operations return optional values. Handle them explicitly:</p> <pre><code>def safe_message_processing(msg: Message) -&gt; Optional[str]:\n    \"\"\"Safely process a message, handling all optional values.\"\"\"\n\n    # Check for errors first\n    if msg.error():\n        return None\n\n    # Handle optional value\n    value: Optional[bytes] = msg.value()\n    if value is None:\n        return None\n\n    try:\n        return value.decode('utf-8')\n    except UnicodeDecodeError:\n        return None\n\ndef process_with_headers(msg: Message) -&gt; Dict[str, str]:\n    \"\"\"Process message headers safely.\"\"\"\n    result: Dict[str, str] = {}\n\n    headers: Optional[List[tuple[str, bytes]]] = msg.headers()\n    if headers:\n        for key, value in headers:\n            try:\n                result[key] = value.decode('utf-8')\n            except UnicodeDecodeError:\n                result[key] = f\"&lt;binary data: {len(value)} bytes&gt;\"\n\n    return result\n</code></pre>"},{"location":"work_with_typings/#common-type-patterns","title":"Common Type Patterns","text":""},{"location":"work_with_typings/#callback-functions","title":"Callback Functions","text":"<pre><code>from typing import Callable\n\n# Type alias for delivery callback\nDeliveryCallback = Callable[[Optional[Exception], Message], None]\n\ndef create_logging_callback(logger_name: str) -&gt; DeliveryCallback:\n    \"\"\"Create a delivery callback with proper typing.\"\"\"\n    import logging\n    logger = logging.getLogger(logger_name)\n\n    def callback(err: Optional[Exception], msg: Message) -&gt; None:\n        if err:\n            logger.error(f\"Message delivery failed: {err}\")\n        else:\n            logger.info(f\"Message delivered to {msg.topic()}[{msg.partition()}]\")\n\n    return callback\n\n# Usage\ncallback: DeliveryCallback = create_logging_callback('kafka.producer')\nproducer.produce('topic', 'message', on_delivery=callback)\n</code></pre> <p>This comprehensive typing approach ensures that your Kafka applications are robust, maintainable, and benefit from excellent IDE support and static analysis.</p>"}]}